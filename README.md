# Non-Supervised-Learning
Machine Learning - Non supervised - Hierarchical clustering on the Habxy(2001) dataset – Report
						

The aim of this project is to test whether a clear difference in pattern activations can be seen on fMRI time series between two different conditions: seeing either animate or inanimate objects. For this purpose I used an unsupervised learning algorithm (hierarchical agglutination clustering) on a dataset (Haxby 2001) from the open-science neuroinformatics database OpenNeuro.org [1].


Background, description of the dataset and hypothesis

James V. Haxby et al [2] tried determining the large-scale spatial organization for this specialization using fMRI and found that the representations of both faces and objects are widely distributed and overlapping in the ventral temporal (VT) cortex. In his experiment he recorded the hemodynamic response of 6 subjects with a GE 3T scanner (TR=2500 ms, TE=90 ms, flip angle=90, 40 sagittal slices of 3,5 mm thickness). The subjects were shown 8 categories of stimuli: faces, houses, cats, bottles, shoes, scissors, chairs, and scrambled images. In total there were twelve series recorded for every subject, in each series containing eight stimulus blocks of 24s each separated by 12s intervals of rest. 
The same dataset was later used by Hanson et al [3], who aimed at testing Haxby’s hypothesis using a neural network classifier containing one hidden layer and reached a similar result, namely an overlap of activation patterns for the categories and lack of single category representations. After training the neural network, the data of the hidden layer was divided into clusters according to the hierarchical agglutination algorithm and they were presented in the form of a dendrogram, which suggested that the neural network “learned” to differentiate between animate (faces, cats) and inanimate (all other) stimuli. This hints towards the fact that fMRI signals could pick up on semantic differences between these categories, which raises the question of whether these differences can be spotted by clustering algorithms from the time series rather than from the information processed in the neural network. Therefore, the hypothesis of this project is that using hierarchical agglutination on the time-series will divide the data into two main subclusters, one containing time series of subjects seeing images of animate objects (faces and cats) and the other subcluster containing the time series of subjects seeing images of inanimate objects (the other stimuli in the Haxby experiment). 


Algorithm

To test this hypothesis, I designed the program to draw a dendrogram for any individual subject based on the similarities of the time-series signals of the VT area voxels computed through the hierarchical clustering algorithm.
The pre-processing of the data consisted of spatial smoothing, time course normalization and applying a VT mask (contained in the Haxby dataset). A 7 x V matrix was built for each subject where 7 represents the number of stimuli and V the number of voxels analysed. Each row represented the mean value of all time-series performed when the subject was watching a particular stimulus, for example the first row (row 0) represent the mean of the time-series acquired as the subject was seeing images of a face. 
The unsupervised clustering algorithm used in this project is hierarchical agglutination clustering. It creates a hierarchy of n+(n-1) clusters from n data points (n this case n=7 data points, each data point consisting of a vector of V length). Starting with an initial set of n cluster (each data point being its own cluster), these clusters are built from the bottom up, and with each iteration i, the nearest pair of clusters are connected to form cluster n+i. The algorithm ends when there is only one cluster containing all n data points.  For this project, the agglomerative clustering was performed using scipy.cluster.hierarchy.linkage function, which outputs a (n-1) x 4 linkage matrix. Looking at the i-th row of this matrix, the first two columns show which two clusters were merged together at the i-th iteration, the third column shows the distance between the merged clusters and the fourth column shows how many individual data points are in the newly formed cluster. For this project, all 7 different methods to compute the distances offered by the linkage function were taken into consideration, although they will not be covered int his report for the sake of brevity. The resulting linkage matrices were used to draw dendrograms, which are binary trees in which the leaves are individual instances. The dissimilarity between two different clusters (also known as cophonetic distance) is reflected in the lengts of the dendogram’s ‘branches’ (i.e. the distance between the leaves and the node). [4,5] 

Caveat: It is worth considering that depending on the chosen method to calculate the distance between clusters, the linkage function can yield slightly different results which in turn determine different dendrograms. Also, a dendrogram can facilitate approximating the number of clusters of a previously unorganized data by offering a visual aid, but it does not divide the data itself.


Results and discussion

Running all possible distance methods for each subject rendered 42 different dendrograms.
Out of all 42 possible dendrograms, only 1 showed two distinct clusters, one containing only ‘faces’ and ‘cats’ and the other one the rest, the dissimilarity between these clusters being different depending on the distance method used. As it can be seen, certain distance methods (i.e. “ward”) show low dissimilarity between face and cat and great dissimilarity between the cluster face’+’cat’ and the rest, which could suggest that the clusters for ‘animate’ and ‘inanimate’ objects are distinct. Furthermore, using the ‘centroid’ method for calculating the distances shows great dissimilarity between ‘face’ and all other stimuli, which could hint toward the existence of a “specialized pattern of activation”.  Also, the majority of dendrograms of subjects 1 and 4 show great dissimilarity between house and all other stimuli, which could hint toward a specialized pattern of activation for ‘house’. But all these hints are not backed up by the results of other subjects or of the same subject using a different distance method, which suggests that any possible classification into distinct clusters of these fMRI signals is extremely diverse and one should be wary to use them as evidence towards any semantic distinctions encoded in the time series.


Bibliography:
1.	Visual object recognition - OpenNeuro. https://openneuro.org/datasets/ds000105/versions/00001.
2.	Haxby, J. V. et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science 293, 2425–2430 (2001).
3.	Hanson, S. J., Matsuka, T. & Haxby, J. V. Combinatorial codes in ventral temporal lobe for object recognition: Haxby (2001) revisited: is there a “face” area? NeuroImage 23, 156–166 (2004).
4.	scipy.cluster.hierarchy.linkage - Scipy v 1.5.1 Reference Guide. https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html.
5.	Géron, A. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. (O’Reilly Media, Inc., 2019).
